# CURRENTLY DEPRECATED AS SPARK_SUBMIT IS NOT WORKING AS EXPECTED

# biggest thanks to this guy: https://github.com/mvillarrealb/docker-spark-cluster
version: "3.7"

services:
  master:
    image: frser/spark-master:2.4.4
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
    environment:
      - "SPARK_LOCAL_IP=master"
    # in reality the workers depend on the master,
    # but this way you can start the cluster with docker-compose up master
    depends_on:
      - worker0
      - worker1
      - worker2
  worker0: &worker-default
    image: frser/spark-worker:2.4.4
    ports:
      - "9091:8081"
    environment:
      - SPARK_MASTER=spark://master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=128m
      - SPARK_EXECUTOR_MEMORY=256m
    volumes:
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
  worker1:
    <<: *worker-default
    ports:
      - "9092:8081"
  worker2:
    <<: *worker-default
    ports:
      - "9093:8081"
  jupyter:
    # image from 30/01/2020 at 7:05 am with Spark 2.4.4
    image: jupyter/pyspark-notebook:414b5d749704
    ports:
      - "8888:8888"
      - "4040:4040"
    volumes:
      - ./apps:/home/jovyan/work
      - ./data:/opt/spark-data
    depends_on:
      - master
